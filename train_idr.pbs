#!/bin/bash
#SBATCH --mem 64G
#SBATCH -n 1
#SBATCH --gres gpu:A100:1
#SBATCH --time 01-00:00:00 #Time for the job to run
#SBATCH --job-name long
#SBATCH -p gpu
##SBATCH -p xudong-gpu
#SBATCH -A xudong-lab


module load miniconda3

source activate /cluster/pixstor/xudong-lab/yuexu/conda/envs/dplm_clone

export TORCH_HOME=/cluster/pixstor/xudong-lab/yuexu/torch_cache/
export HF_HOME=/cluster/pixstor/xudong-lab/yuexu/transformers_cache/
export CONDA_PKGS_DIRS=/cluster/pixstor/xudong-lab/yuexu/conda/pkgs 
export CONDA_ENVS_PATH=/cluster/pixstor/xudong-lab/yuexu/conda/envs 
export PIP_CACHE_DIR=/cluster/pixstor/xudong-lab/yuexu/conda/pip_cache



# accelerate launch train.py --config_path ./configs_hell/gvp_v2/config_plddtallweight_noseq_v2.yaml \
# --result_path ./results/plddtallweight_noseq_v2/
accelerate launch train_idr.py --config_path $config_path --result_path $result_path --resume_path $model_path

#accelerate launch train_mdcath.py --config_path $config_path --result_path $result_path
