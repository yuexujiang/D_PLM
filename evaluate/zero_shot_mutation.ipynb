{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa48d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/cluster/pixstor/xudong-lab/yuexu/SeqDance-main/SeqDance-main/model/\")\n",
    "from config import config # please first download the dataset and fill in the config.py file with the path where you downloaded the dataset\n",
    "from model import ESMwrap\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n",
    "\n",
    "# Load model\n",
    "esm2_select = 'model_35M'\n",
    "model_select = 'seqdance' # or 'esmdance'\n",
    "dance_model = ESMwrap(esm2_select, model_select).to(device)\n",
    "\n",
    "# Load the SeqDance model from huggingface\n",
    "dance_model = dance_model.from_pretrained(\"ChaoHou/ESMDance\")\n",
    "dance_model = dance_model.to(device)\n",
    "dance_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bcf4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/cluster/pixstor/xudong-lab/yuexu/SeqDance-main/SeqDance-main/data/Processed_K50_dG_datasets/Processed_K50_dG_datasets/Tsuboyama2023_Dataset2_Dataset3_20230416.csv')\n",
    "\n",
    "# data filtering: 1. have ddG value, 2. not insertion or deletion as the sequence lengths are different\n",
    "df = df[(df['ddG_ML']!='-') & (~df['mut_type'].str.contains('ins|del'))]\n",
    "df['ddG_ML'] = df['ddG_ML'].astype(float)\n",
    "\n",
    "# get the mean ddG value for each aa_seq (some aa_seq have multiple experiments)\n",
    "ddg_mean = df[['aa_seq','ddG_ML']].groupby('aa_seq').mean()\n",
    "df = pd.merge(df.sort_values('mut_type', ascending=False).drop_duplicates('aa_seq')[['aa_seq','mut_type','WT_name','WT_cluster']], ddg_mean, on='aa_seq', how='left')\n",
    "\n",
    "df.index = df['WT_name'] + '$' + df['mut_type']\n",
    "\n",
    "pro = 'r10_572_TrROS_Hall.pdb'\n",
    "df_pro = df[df['WT_name'] == pro]\n",
    "\n",
    "# get the prediction for the wildtype sequence\n",
    "wt_seq = df.loc[f'{pro}$wt','aa_seq']\n",
    "wt_input = tokenizer(wt_seq, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    wt_output = dance_model(wt_input)\n",
    "\n",
    "# all mutations\n",
    "pro_muts = df[df['WT_name']==pro]['mut_type']\n",
    "\n",
    "# the epsilon is used to avoid division by zero\n",
    "epsilon = 1e-2\n",
    "\n",
    "for mt in pro_muts:\n",
    "    # get the prediction for the mutant sequence\n",
    "    mt_seq = df.loc[f'{pro}${mt}','aa_seq']\n",
    "    mt_input = tokenizer(mt_seq, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        mt_output = dance_model(mt_input)\n",
    "    # calculate the relative difference between the mutant and wildtype predictions\n",
    "    for feature in wt_output:\n",
    "        df_pro.loc[f'{pro}${mt}',feature] = (abs(mt_output[feature] - wt_output[feature]) / (abs(wt_output[feature]) + epsilon)).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_normalization(features):\n",
    "    \"\"\"\n",
    "    Applies quantile normalization to a set of feature vectors (each column represents a protein).\n",
    "    Ensures all features have the same distribution across proteins.\n",
    "    \"\"\"\n",
    "    features = np.array(features)\n",
    "    \n",
    "    # Rank transformation\n",
    "    ranks = np.argsort(np.argsort(features, axis=0), axis=0)\n",
    "    \n",
    "    # Compute mean per rank across all features\n",
    "    sorted_features = np.sort(features, axis=0)\n",
    "    rank_means = np.mean(sorted_features, axis=1)\n",
    "    \n",
    "    # Map original values to rank means\n",
    "    normalized_matrix = np.zeros_like(features, dtype=np.float64)\n",
    "    for i in range(features.shape[1]):\n",
    "        normalized_matrix[:,i] = rank_means[ranks[:,i]]\n",
    "    \n",
    "    return normalized_matrix\n",
    "\n",
    "def get_normalized_values(df, features):\n",
    "    \"\"\"\n",
    "    Normalizes the values of the features to have the same distribution across proteins.\n",
    "    \"\"\"\n",
    "    normalized_value = quantile_normalization(df[features].values)\n",
    "    normalized_df = pd.DataFrame(normalized_value, columns=[i + '_norm' for i in features])\n",
    "    normalized_df.index = df.index\n",
    "    df = pd.merge(df, normalized_df, left_index=True, right_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e92c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_pro.columns[5:]\n",
    "df_pro = get_normalized_values(df_pro, features)\n",
    "\n",
    "# Convert to NumPy array\n",
    "raw_features = df_pro[features].values\n",
    "normalized_features = df_pro[[i + '_norm' for i in features]].values\n",
    "\n",
    "# Compute row-wise weights (value / sum of row)\n",
    "raw_weights = (raw_features + 1e-8) / (raw_features + 1e-8).sum(axis=1, keepdims=True)\n",
    "norm_weights = (normalized_features + 1e-8) / (normalized_features + 1e-8).sum(axis=1, keepdims=True)\n",
    "\n",
    "# Compute feature combinations\n",
    "df_pro['raw_mean'] = raw_features.mean(axis=1)\n",
    "df_pro['raw_max'] = raw_features.max(axis=1)\n",
    "df_pro['raw_weighted_mean'] = np.sum(raw_features * raw_weights, axis=1)\n",
    "df_pro['raw_geometric_mean'] = np.exp(np.mean(np.log(raw_features + 1e-8), axis=1))\n",
    "\n",
    "df_pro['norm_mean'] = normalized_features.mean(axis=1)\n",
    "df_pro['norm_max'] = normalized_features.max(axis=1)\n",
    "df_pro['norm_weighted_mean'] = np.sum(normalized_features * norm_weights, axis=1)\n",
    "df_pro['norm_geometric_mean'] = np.exp(np.mean(np.log(normalized_features + 1e-8), axis=1))\n",
    "\n",
    "df_pro.iloc[:, 4:].corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import esm\n",
    "# Load the ESM-2 model\n",
    "esm_35, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "esm_35 = esm_35.to(device)\n",
    "esm_35.eval()\n",
    "# Create a batch converter\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "# use wildtype sequence to get the logits, then get LLR, this perform similar to masked LLR\n",
    "def get_wt_logits(esm_model, batch_converter, seq):\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter([('name', seq)])\n",
    "    with torch.no_grad():\n",
    "        results = esm_model(batch_tokens.to(device))\n",
    "    logits = torch.log_softmax(results[\"logits\"],dim=-1)[0,:,:].cpu().numpy()[1:-1,:]\n",
    "    return logits\n",
    "\n",
    "# mask each residue, get the logits of masked residue, then get LLR\n",
    "def get_mask_llr(esm_model, batch_converter, alphabet, seq):\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter([('name', seq)])\n",
    "    mask_logits = []\n",
    "    for i in range(len(seq)):\n",
    "        batch_tokens_masked = batch_tokens.clone()\n",
    "        batch_tokens_masked[0, i+1] = alphabet.mask_idx\n",
    "        with torch.no_grad():\n",
    "            results = esm_model(batch_tokens_masked.to(device))\n",
    "        mask_logits.append(results[\"logits\"][:,i+1,:])\n",
    "    \n",
    "    mask_logits = torch.cat(mask_logits, dim=0)\n",
    "    mask_logits = torch.log_softmax(mask_logits,dim=-1).cpu().numpy()\n",
    "\n",
    "    return mask_logits\n",
    "\n",
    "def logits_to_llr(logits, alphabet, seq):\n",
    "    logits = pd.DataFrame(logits,columns=alphabet.all_toks,index=list(seq)).T\n",
    "    wt_norm=np.diag(logits.loc[logits.columns])\n",
    "    llr = logits-wt_norm\n",
    "    \n",
    "    llr.columns = [seq[i] + str(i+1) for i in range(len(seq))]\n",
    "    \n",
    "    llr = pd.DataFrame(llr.iloc[4:24].T.stack(), columns=['LLR']).reset_index()\n",
    "    llr['mutant'] = llr['level_0'].str.replace('_','') + llr['level_1']\n",
    "    return llr[['mutant','LLR']].set_index('mutant')\n",
    "\n",
    "# get the zero-shot LLR using both wildtype and masked logits\n",
    "wt_logits = get_wt_logits(esm_35, batch_converter, wt_seq)\n",
    "wt_llr = logits_to_llr(wt_logits, alphabet, wt_seq)\n",
    "\n",
    "mask_logits = get_mask_llr(esm_35, batch_converter, alphabet, wt_seq)\n",
    "mask_llr = logits_to_llr(mask_logits, alphabet, wt_seq)\n",
    "for mt in pro_muts:\n",
    "    if mt != 'wt':\n",
    "        # merge the LLR values if there are multiple mutations\n",
    "        df_pro.loc[f'{pro}${mt}', 'wt_llr'] = wt_llr.loc[mt.split(':'), 'LLR'].sum()\n",
    "        df_pro.loc[f'{pro}${mt}', 'mask_llr'] = mask_llr.loc[mt.split(':'), 'LLR'].sum()\n",
    "    else:\n",
    "        df_pro.loc[f'{pro}${mt}', 'wt_llr'] = 0\n",
    "        df_pro.loc[f'{pro}${mt}', 'mask_llr'] = 0\n",
    "\n",
    "df_pro.iloc[:,4:].corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################DPLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8222a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "import esm\n",
    "import esm_adapterH\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "from utils.utils import load_configs\n",
    "from collections import OrderedDict\n",
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "\n",
    "def load_checkpoints(model,checkpoint_path):\n",
    "        if checkpoint_path is not None:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
    "            if  any('adapter' in name for name, _ in model.named_modules()):\n",
    "                if np.sum([\"adapter_layer_dict\" in key for key in checkpoint[\n",
    "                    'state_dict1'].keys()]) == 0:  # using old checkpoints, need to rename the adapter_layer into adapter_layer_dict.adapter_0\n",
    "                    new_ordered_dict = OrderedDict()\n",
    "                    for key, value in checkpoint['state_dict1'].items():\n",
    "                        if \"adapter_layer_dict\" not in key:\n",
    "                            new_key = key.replace('adapter_layer', 'adapter_layer_dict.adapter_0')\n",
    "                            new_ordered_dict[new_key] = value\n",
    "                        else:\n",
    "                            new_ordered_dict[key] = value\n",
    "                    \n",
    "                    model.load_state_dict(new_ordered_dict, strict=False)\n",
    "                else:\n",
    "                    #this model does not contain esm2\n",
    "                    new_ordered_dict = OrderedDict()\n",
    "                    for key, value in checkpoint['state_dict1'].items():\n",
    "                            key = key.replace(\"esm2.\",\"\")\n",
    "                            new_ordered_dict[key] = value\n",
    "                    \n",
    "                    model.load_state_dict(new_ordered_dict, strict=False)\n",
    "            elif  any('lora' in name for name, _ in model.named_modules()):\n",
    "                #this model does not contain esm2\n",
    "                new_ordered_dict = OrderedDict()\n",
    "                for key, value in checkpoint['state_dict1'].items():\n",
    "                        print(key)\n",
    "                        key = key.replace(\"esm2.\",\"\")\n",
    "                        new_ordered_dict[key] = value\n",
    "                \n",
    "                model.load_state_dict(new_ordered_dict, strict=False)\n",
    "            \n",
    "            print(\"checkpoints were loaded from \" + checkpoint_path)\n",
    "        else:\n",
    "            print(\"checkpoints not exist \"+ checkpoint_path)\n",
    "\n",
    "# def load_model(args):\n",
    "#     if args.model_type==\"d-plm\":\n",
    "#         with open(args.config_path) as file:\n",
    "#             config_file = yaml.full_load(file)\n",
    "#             configs = load_configs(config_file, args=None)\n",
    "        \n",
    "#         # inference for each model\n",
    "#         model, alphabet = esm_adapterH.pretrained.esm2_t33_650M_UR50D(configs.model.esm_encoder.adapter_h)\n",
    "#         load_checkpoints(model,args.model_location)\n",
    "#     elif args.model_type==\"ESM2\":\n",
    "#         #if use ESM2\n",
    "#         model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    \n",
    "    \n",
    "#     model.eval()\n",
    "#     if torch.cuda.is_available():\n",
    "#         model = model.cuda()\n",
    "#         print(\"Transferred model to GPU\")\n",
    "    \n",
    "#     return model,alphabet\n",
    "\n",
    "def load_model(args):\n",
    "    if args.model_type==\"d-plm\":\n",
    "        with open(args.config_path) as file:\n",
    "            config_file = yaml.full_load(file)\n",
    "            configs = load_configs(config_file, args=None)\n",
    "        if configs.model.esm_encoder.adapter_h.enable:\n",
    "            model, alphabet = esm_adapterH.pretrained.esm2_t33_650M_UR50D(configs.model.esm_encoder.adapter_h)\n",
    "        elif configs.model.esm_encoder.lora.enable:\n",
    "            model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "            lora_targets =  [\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\",\"self_attn.out_proj\"]\n",
    "            target_modules=[]\n",
    "            if configs.model.esm_encoder.lora.esm_num_end_lora > 0:\n",
    "                start_layer_idx = np.max([model.num_layers - configs.model.esm_encoder.lora.esm_num_end_lora, 0])\n",
    "                for idx in range(start_layer_idx, model.num_layers):\n",
    "                    for layer_name in lora_targets:\n",
    "                        target_modules.append(f\"layers.{idx}.{layer_name}\")\n",
    "                \n",
    "            peft_config = LoraConfig(\n",
    "                inference_mode=False,\n",
    "                r=configs.model.esm_encoder.lora.r,\n",
    "                lora_alpha=configs.model.esm_encoder.lora.alpha,\n",
    "                target_modules=target_modules,\n",
    "                lora_dropout=configs.model.esm_encoder.lora.dropout,\n",
    "                bias=\"none\",\n",
    "                # modules_to_save=modules_to_save\n",
    "            )\n",
    "            peft_model = get_peft_model(model, peft_config)\n",
    "        \n",
    "        # inference for each model\n",
    "        # model, alphabet = esm_adapterH.pretrained.esm2_t33_650M_UR50D(configs.model.esm_encoder.adapter_h)/\n",
    "        load_checkpoints(model,args.model_location)\n",
    "    elif args.model_type==\"ESM2\":\n",
    "        #if use ESM2\n",
    "        model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        print(\"Transferred model to GPU\")\n",
    "    \n",
    "    return model,alphabet\n",
    "\n",
    "def main(args):\n",
    "    batch_converter = args.alphabet.get_batch_converter()\n",
    "    \n",
    "    data = [\n",
    "        (\"protein1\", args.sequence),\n",
    "    ]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        wt_representation = args.model(batch_tokens.cuda(),repr_layers=[args.model.num_layers])[\"representations\"][args.model.num_layers]\n",
    "    \n",
    "    wt_representation = wt_representation.squeeze(0) #only one sequence a time\n",
    "    return wt_representation\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch SimCLR')\n",
    "parser.add_argument(\"--model-location\", type=str, help=\"xx\", default='/cluster/pixstor/xudong-lab/yuexu/D_PLM/results/vivit_3/checkpoints/checkpoint_best_val_rmsf_cor.pth')\n",
    "parser.add_argument(\"--config-path\", type=str, default='/cluster/pixstor/xudong-lab/yuexu/D_PLM/results/vivit_3/config_vivit3.yaml', help=\"xx\")\n",
    "parser.add_argument(\"--model-type\", default='d-plm', type=str, help=\"xx\")\n",
    "parser.add_argument(\"--sequence\", type=str, help=\"xx\")\n",
    "parser.add_argument(\"--output_path\", type=str, default='/cluster/pixstor/xudong-lab/yuexu/D_PLM/evaluate/', help=\"xx\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.model,args.alphabet = load_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e97676",
   "metadata": {},
   "outputs": [],
   "source": [
    "############zero shot\n",
    "pro_id_set=set()\n",
    "for i in df.index:\n",
    "    pro_id_set.add(i.split(\"$\")[0])\n",
    "\n",
    "corr_list=[]\n",
    "\n",
    "for pro in pro_id_set:\n",
    "    # pro = 'r10_572_TrROS_Hall.pdb'\n",
    "    df_pro = df[df['WT_name'] == pro]\n",
    "    # get the prediction for the wildtype sequence\n",
    "    wt_seq = df.loc[f'{pro}$wt','aa_seq']\n",
    "    # wt_input = tokenizer(wt_seq, return_tensors=\"pt\").to(device)\n",
    "    # with torch.no_grad():\n",
    "    #     wt_output = dance_model(wt_input)\n",
    "    args.sequence=wt_seq\n",
    "    wt_output = main(args)\n",
    "    wt_output = wt_output[1:-1]\n",
    "    # all mutations\n",
    "    pro_muts = df[df['WT_name']==pro]['mut_type']\n",
    "    # the epsilon is used to avoid division by zero\n",
    "    epsilon = 1e-2\n",
    "    ddG_list=[]\n",
    "    score_list=[]\n",
    "    for mt in pro_muts:\n",
    "        if mt==\"wt\":\n",
    "            continue\n",
    "        # get the prediction for the mutant sequence\n",
    "        mt_seq = df.loc[f'{pro}${mt}','aa_seq']\n",
    "        # mt_input = tokenizer(mt_seq, return_tensors=\"pt\").to(device)\n",
    "        args.sequence=mt_seq\n",
    "        mt_output = main(args)\n",
    "        mt_output = mt_output[1:-1]\n",
    "        score = np.linalg.norm((mt_output-wt_output).to('cpu').detach().numpy(),axis=1)\n",
    "        score = np.log(np.mean(score))*-1\n",
    "        score_list.append(score)\n",
    "        ddG_list.append(df.loc[f'{pro}${mt}','ddG_ML'])\n",
    "    \n",
    "    print(\"done\")\n",
    "    corr, _ = spearmanr(ddG_list, score_list)\n",
    "    corr_list.append(corr)\n",
    "\n",
    "np.mean(corr_list) #0.441\n",
    "\n",
    "np.median(corr_list) #0.456\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf413cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81472e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression each pro 50% for train\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import spearmanr\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "pro_id_set=set()\n",
    "for i in df.index:\n",
    "    pro_id_set.add(i.split(\"$\")[0])\n",
    "    \n",
    "# ============================================================================\n",
    "# STANDALONE VERSION (copy-paste directly into your code)\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Collect data\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "print(\"Processing proteins and collecting mutations...\")\n",
    "for pro in tqdm(list(pro_id_set)):\n",
    "    # Get wild-type embedding\n",
    "    wt_seq = df.loc[f'{pro}$wt', 'aa_seq']\n",
    "    args.sequence = wt_seq\n",
    "    wt_output = main(args)\n",
    "    wt_emb = wt_output[1:-1].cpu().numpy()  # Remove BOS/EOS\n",
    "    \n",
    "    # Get all mutations (excluding wildtype)\n",
    "    pro_muts = df[df['WT_name']==pro]['mut_type'].values\n",
    "    pro_muts = [mt for mt in pro_muts if mt != \"wt\"]\n",
    "    \n",
    "    if len(pro_muts) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Shuffle and split mutations for this protein (50/50)\n",
    "    pro_muts_shuffled = np.random.permutation(pro_muts)\n",
    "    n_train = len(pro_muts_shuffled) // 2\n",
    "    train_muts = pro_muts_shuffled[:n_train]\n",
    "    test_muts = pro_muts_shuffled[n_train:]\n",
    "    \n",
    "    # Process training mutations\n",
    "    for mt in train_muts:\n",
    "        mt_seq = df.loc[f'{pro}${mt}', 'aa_seq']\n",
    "        args.sequence = mt_seq\n",
    "        mt_output = main(args)\n",
    "        mt_emb = mt_output[1:-1].cpu().numpy()\n",
    "        \n",
    "        feature = np.mean(mt_emb - wt_emb, axis=0)\n",
    "        ddG = df.loc[f'{pro}${mt}', 'ddG_ML']\n",
    "        \n",
    "        X_train_list.append(feature)\n",
    "        y_train_list.append(ddG)\n",
    "    \n",
    "    # Process test mutations\n",
    "    for mt in test_muts:\n",
    "        mt_seq = df.loc[f'{pro}${mt}', 'aa_seq']\n",
    "        args.sequence = mt_seq\n",
    "        mt_output = main(args)\n",
    "        mt_emb = mt_output[1:-1].cpu().numpy()\n",
    "        \n",
    "        feature = np.mean(mt_emb - wt_emb, axis=0)\n",
    "        ddG = df.loc[f'{pro}${mt}', 'ddG_ML']\n",
    "        \n",
    "        X_test_list.append(feature)\n",
    "        y_test_list.append(ddG)\n",
    "\n",
    "# Convert to arrays\n",
    "X_train = np.stack(X_train_list)\n",
    "y_train = np.array(y_train_list)\n",
    "X_test = np.stack(X_test_list)\n",
    "y_test = np.array(y_test_list)\n",
    "\n",
    "print(f\"\\\\nTraining mutations: {len(y_train)}\")\n",
    "print(f\"Test mutations: {len(y_test)}\")\n",
    "\n",
    "# Train and evaluate\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_spearman, _ = spearmanr(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_spearman, _ = spearmanr(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"RESULTS - Per-Protein Mutation Split\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training Set: MAE={train_mae:.4f}, Spearman={train_spearman:.4f}\")\n",
    "print(f\"Test Set:     MAE={test_mae:.4f}, Spearman={test_spearman:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression repeat 20 times\n",
    "\n",
    "# ============================================================================\n",
    "# MINIMAL STANDALONE VERSION (copy-paste into your code)\n",
    "# ============================================================================\n",
    "\n",
    "# Step 1: Cache embeddings ONCE\n",
    "embeddings = {}\n",
    "for pro in tqdm(list(pro_id_set), desc=\"Caching embeddings\"):\n",
    "    embeddings[pro] = {'mutations': {}}\n",
    "    \n",
    "    # Wildtype\n",
    "    wt_seq = df.loc[f'{pro}$wt', 'aa_seq']\n",
    "    args.sequence = wt_seq\n",
    "    wt_output = main(args)\n",
    "    # embeddings[pro]['wt'] = wt_output[1:-1].cpu().numpy()\n",
    "    wt_emb = wt_output[1:-1].cpu().numpy()\n",
    "    \n",
    "    # Mutations\n",
    "    for mt in df[df['WT_name']==pro]['mut_type'].values:\n",
    "        if mt == \"wt\":\n",
    "            continue\n",
    "        mt_seq = df.loc[f'{pro}${mt}', 'aa_seq']\n",
    "        args.sequence = mt_seq\n",
    "        mt_output = main(args)\n",
    "        # embeddings[pro]['mutations'][mt] = mt_output[1:-1].cpu().numpy()\n",
    "        mt_emb = mt_output[1:-1].cpu().numpy()\n",
    "        embeddings[pro]['mutations'][mt] = np.mean(mt_emb - wt_emb, axis=0)\n",
    "\n",
    "# Step 2: Run 20 times\n",
    "results = []\n",
    "for run_i in tqdm(range(20), desc=\"Running 20 splits\"):\n",
    "    np.random.seed(42 + run_i)\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = [], [], [], []\n",
    "    \n",
    "    for pro in list(pro_id_set):\n",
    "        # wt_emb = embeddings[pro]['wt']\n",
    "        pro_muts = [mt for mt in df[df['WT_name']==pro]['mut_type'].values if mt != \"wt\"]\n",
    "        \n",
    "        if len(pro_muts) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Split 50/50\n",
    "        pro_muts_shuffled = np.random.permutation(pro_muts)\n",
    "        n_train = len(pro_muts_shuffled) // 2\n",
    "        \n",
    "        for mt in pro_muts_shuffled[:n_train]:\n",
    "            # mt_emb = embeddings[pro]['mutations'][mt]\n",
    "            # X_train.append(np.mean(mt_emb - wt_emb, axis=0))\n",
    "            X_train.append(embeddings[pro]['mutations'][mt])\n",
    "            y_train.append(df.loc[f'{pro}${mt}', 'ddG_ML'])\n",
    "        \n",
    "        for mt in pro_muts_shuffled[n_train:]:\n",
    "            # mt_emb = embeddings[pro]['mutations'][mt]\n",
    "            # X_test.append(np.mean(mt_emb - wt_emb, axis=0))\n",
    "            X_test.append(embeddings[pro]['mutations'][mt])\n",
    "            y_test.append(df.loc[f'{pro}${mt}', 'ddG_ML'])\n",
    "    \n",
    "    # Train and evaluate\n",
    "    X_train = np.stack(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.stack(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    spearman, _ = spearmanr(y_test, y_pred)\n",
    "    \n",
    "    results.append({'run': run_i, 'test_mae': mae, 'test_spearman': spearman})\n",
    "\n",
    "# Step 3: Summarize\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\\\nTest MAE: {results_df['test_mae'].mean():.4f} ± {results_df['test_mae'].std():.4f}\")\n",
    "print(f\"Test Spearman: {results_df['test_spearman'].mean():.4f} ± {results_df['test_spearman'].std():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
